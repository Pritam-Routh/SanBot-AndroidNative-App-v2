# Project-wide Gradle settings.

# IDE (e.g. Android Studio) users:
# Gradle settings configured through the IDE *will override*
# any settings specified in this file.

# For more details on how to configure your build environment visit
# http://www.gradle.org/docs/current/userguide/build_environment.html

# Specifies the JVM arguments used for the daemon process.
# The setting is particularly useful for tweaking memory settings.
org.gradle.jvmargs=-Xmx4096m -Dfile.encoding=UTF-8

# When configured, Gradle will run in incubating parallel mode.
# This option should only be used with decoupled projects.
org.gradle.parallel=true

# AndroidX package structure
android.useAndroidX=true

# Kotlin code style
kotlin.code.style=official

# Enable build cache
org.gradle.caching=true

# Non-transitive R classes
android.nonTransitiveRClass=true

# ============================================
# SANBOT VOICE AGENT CONFIGURATION
# ============================================

# Backend API Configuration
# TEST BACKEND (temporary - for development)
TRIPANDEVENT_BASE_URL=https://p88hmxbz-3051.inc1.devtunnels.ms

# PRODUCTION BACKEND (uncomment when ready)
# TRIPANDEVENT_BASE_URL=https://bot.tripandevent.com
TRIPANDEVENT_API_TOKEN=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ0eXBlIjoidm9pY2Vib3QiLCJuYW1lIjoiUmVhbHRpbWVfdm9pY2VfQXBpX1Rlc3QiLCJpYXQiOjE3NjU1ODcyNjcsImV4cCI6MTc5NzEyMzI2N30.wifip7QzWqtqnlCQQSiuS0DuBplI_dM_dPYqS4nwSlk

# OpenAI Configuration (DO NOT PUT API KEY HERE - use ephemeral tokens)
OPENAI_REALTIME_MODEL=gpt-realtime
OPENAI_VOICE=marin

# Feature Flags
ENABLE_TRANSCRIPT_DISPLAY=true
ENABLE_DEBUG_LOGGING=true

# HeyGen Configuration (disabled - using LiveAvatar instead)
ENABLE_HEYGEN_AVATAR=false
HEYGEN_AVATAR_ID=

# ============================================
# OPENAI CONNECTION MODE
# ============================================
# true = WebSocket mode (enables CUSTOM mode for avatar, provides audio data access)
# false = WebRTC mode (native audio handling, no audio data access)
#
# WebSocket mode:
# - Provides response.output_audio.delta events with raw PCM audio
# - Enables CUSTOM mode for LiveAvatar (direct audio streaming for lip sync)
# - LOWEST LATENCY for avatar lip sync
# - Requires manual audio playback (handled by OpenAIWebSocketManager)
#
# WebRTC mode:
# - Audio handled natively by WebRTC (echo cancellation, jitter buffering)
# - No raw audio data access - only text transcripts available
# - Avatar uses FULL mode (text-based TTS)
# - Higher latency for avatar lip sync
OPENAI_USE_WEBSOCKET=true

# ============================================
# LIVEAVATAR CONFIGURATION (Audio-based, lowest latency)
# ============================================
# Enable LiveAvatar (replaces HeyGen when true)
ENABLE_LIVEAVATAR=true
# LiveAvatar avatar ID (get from LiveAvatar dashboard)
LIVEAVATAR_AVATAR_ID=7299c55d-1f45-482d-915c-e5efdc9dd266
# Use audio streaming mode for lowest latency (true = audio deltas, false = text)
# IMPORTANT: Audio streaming (CUSTOM mode) requires OPENAI_USE_WEBSOCKET=true
# When OPENAI_USE_WEBSOCKET=false (WebRTC mode), this setting is ignored and FULL mode is used.
LIVEAVATAR_AUDIO_STREAMING=true

# ============================================
# TEXT BUFFER CONFIGURATION (Dynamic batching)
# ============================================
# Dynamic batching mimics OpenAI Realtime behavior:
# 1. FIRST FLUSH: Eager (3 words, 150ms) - start avatar speaking quickly
# 2. SUBSEQUENT: Sentence-based or clause-based for smooth speech
# 3. INTERRUPT: User speech clears buffer and resets state
#
# These settings only affect SUBSEQUENT flushes (after first flush)

# Batch delay before flushing mid-sentence (ms) - for subsequent flushes
TEXT_DELTA_BATCH_DELAY_MS=300

# Minimum words for mid-sentence flush - for subsequent flushes
TEXT_DELTA_MIN_WORDS=6